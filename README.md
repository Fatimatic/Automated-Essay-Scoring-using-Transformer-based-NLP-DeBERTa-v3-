# Automated-Essay-Scoring-using-Transformer-based-NLP-DeBERTa-v3-
Built an automated essay scoring system using DeBERTa v3 and transformer-based NLP, predicting 6 analytic writing rubrics with low error using MCRMSE evaluation.


# Automated Essay Scoring using DeBERTa v3 (Transformer-based NLP)

## ğŸ“Œ Project Overview
Manual essay grading is time-consuming, subjective, and often biased toward essay length rather than writing quality.  
This project presents an **Automated Essay Scoring (AES)** system using **transformer-based NLP** to evaluate essays consistently and fairly across multiple writing dimensions.

I built a deep learning model using **DeBERTa v3 Small**, a state-of-the-art transformer architecture, to predict **six analytic writing rubrics** simultaneously.

---

## ğŸ¯ Problem Statement
Human graders:
- May unintentionally favor longer essays
- Can be inconsistent due to fatigue or bias
- Struggle to score large volumes efficiently

The goal was to **automate essay scoring** while focusing on **quality, structure, and language proficiency** rather than content length.

---

## ğŸ“Š Dataset
- **Dataset Name:** ELLIPSE Corpus (Kaggle)
- **Essay Type:** Argumentative essays
- **Authors:** Grade 8â€“12 English Language Learners (ELLs)
- **Scoring Rubrics (Targets):**
  - Cohesion
  - Syntax
  - Vocabulary
  - Phraseology
  - Grammar
  - Conventions
- **Score Range:** 1.0 â€“ 5.0 (increments of 0.5)

âš ï¸ The full dataset is **not included** due to size and redistribution restrictions.  
A small sample dataset is provided for demonstration purposes.

---


Raw Essay Text
â†“
Text Cleaning & Preparation
â†“
Baseline (TF-IDF) Experiment
â†“
Tokenizer (DeBERTa Tokenizer)
â†“
Transformer Encoder (DeBERTa v3 Small)
â†“
CLS Token Embedding
â†“
Linear Regression Head
â†“
Six Rubric Score Predictions



---

## ğŸ§  Why Not TF-IDF?
TF-IDF treats text as isolated word frequencies and **ignores context, grammar, and sentence structure**.  
Essay quality depends on **semantic coherence and syntax**, which require contextual understanding.

Hence,I moved to **transformer-based contextual embeddings**.

---

## ğŸ— Model Architecture

- **Backbone:** DeBERTa v3 Small (Transformer Encoder)
- **Embedding Size:** 768
- **Pooling Strategy:** CLS token representation
- **Head:** Fully connected linear layer (regression)
- **Dropout:** 0.2 (to reduce overfitting)
- **Output:** 6 continuous values (one per rubric)

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|--------|------|
| Optimizer | AdamW |
| Loss Function | Mean Squared Error (MSE) |
| Evaluation Metric | MCRMSE |
| Epochs | 1 (overfitting observed beyond this) |
| Batch Size | 4 |
| Max Token Length | 128 |
| Device | CPU |

> Training on CPU took approximately **1.5 hours**.

---

## ğŸ“ Evaluation Metric
I used **Mean Column-wise Root Mean Squared Error (MCRMSE)**, which evaluates prediction error independently for each rubric and then averages them.

RÂ² was not used due to **low variance in rubric scores**.

---

## ğŸš€ Inference (User Input Prediction)
The system allows users to paste an essay and receive predicted rubric scores instantly.

Example output:
Cohesion: 3.50
Syntax: 3.00
Vocabulary: 3.50
Phraseology: 3.00
Grammar: 4.00
Conventions: 4.50



---

## ğŸ“¦ Trained Model
The trained model file (~530MB) is hosted externally due to GitHub size limits.

ğŸ‘‰ **Download model weights here:**  
https://drive.google.com/file/d/1SktU7SaviJmV4Hybuy6XdtrVRdBMs4vj/view?usp=sharing

After downloading, place the file in the project root directory before running inference.

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt

### 2ï¸âƒ£ Train the Model
python training.py

### 3ï¸âƒ£ Run Inference
python inference.py


## ğŸ” Project Workflow

ğŸ”® Future Improvements

Train on GPU for longer context lengths

Use larger DeBERTa variants

Incorporate sentence-level attention analysis

Ensemble multiple transformer models

ğŸ§‘â€ğŸ’» Skills Demonstrated

Transformer-based NLP

Regression modeling

Deep learning optimization

Evaluation metric design

End-to-end ML pipeline

Real-world inference deployment

ğŸ“š Technologies Used

Python

PyTorch

Hugging Face Transformers

DeBERTa v3

Scikit-learn

Pandas & NumPy

â­ Summary

This project demonstrates an end-to-end NLP system that combines modern transformer architectures with educational assessment, highlighting practical machine learning skills under real-world constraints.
